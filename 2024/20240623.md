前陣子看的幾篇，如何得到更好的答案:

[Introducing Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations | Lamini - Enterprise LLM Platform](https://www.lamini.ai/blog/lamini-memory-tuning)

[Banishing LLM Hallucinations Requires Rethinking Generalization](https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf) (論文)

Lamini 提出 MoME (Mixture of Memory Experts)，可用一百萬個專家，每個專家記住一個事實，對於該事實可以保證 100% 正確。相較 (他們測試中的) RAG 之 50% 正確率，Lamini 在 8 個 AMD MI300X GPU 上訓練 1 小時即可提升到 95%，而且對其他一般能力並無明顯影響。
![](images/20240623193101.png)
![](images/20240623193203.png)

---
[From grep to SPLADE: a journey through semantic search](https://blog.elicit.com/semantic-search/)

SPLADE 讓 LLM 自動擴展使用者的輸入詞語，再用傳統的全文搜尋找答案。這樣做不但速度快，中間步驟可驗證，還能保證搜尋結果可精確重現。對於論文搜尋，提出 ACS (Automated Comprehensive Search)，依相關性進行排序，讓 LLM 依序讀取，可以在機率上保證已讀取所有相關論文。

![](images/20240623221525.png)

[AI Search: The Bitter-er Lesson](https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d)

舉西洋棋為例，使用人類規則加上小模型的 Stockfish 勝過完全用大模型的 Leela。指出在發散的空間中進行有效率的搜尋，可以在有限的計算量中達到更好的結果，不要將所有資源都投入更大的參數模型上，使用目前的模型就有可能達成。