12/15-12/31 AI 新知

語言
----
[OpenAI Begins Tackling ChatGPT Data Leak Vulnerability](https://embracethered.com/blog/posts/2023/openai-data-exfiltration-first-mitigations-implemented/) 攻擊 ChatGPT 之問題已開始解決，但仍不完美

[An In-depth Look at Gemini's Language Abilities](https://browse.arxiv.org/html/2312.11444v2) 評測說 Gemini Pro 英文略遜於 GPT-3.5 Turbo，但翻譯能力較強

[Phi-2: The surprising power of small language models - Microsoft Research](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) 微軟推出的 2.7B 小模型，某些方面能力可與 25 倍大的模型相當

[(Long)LLMLingua | Designing a Language for LLMs via Prompt Compression](https://llmlingua.com/) [開源][論文] 微軟使用小模型將輸入文字壓縮 (去掉不重要部份)，效能損失極微，壓縮率可高達 20 倍! 可用於輸入更長文本，減少成本、加快速度，正確率甚至還可能提升!

[How we built "Mistral 7B Fine-Tune Optimized," the best 7B model for fine-tuning](https://openpipe.ai/blog/mistral-7b-fine-tune-optimized) [開源] Mistral 7B 微調模型竟可超越 GPT-4! 合併兩個模型可以獲得部份甚至全部能力!

[yule-BUAA/MergeLM: Codebase for Merging Language Models](https://github.com/yule-BUAA/MergeLM/) [開源] 可將模型 90~99% 參數設為零不影響效能，再將同源不同模型以參數平均合併為新模型，一次獲得多個模型的能力

[SJTU-IPADS/PowerInfer: High-speed Large Language Model Serving on PCs with Consumer-grade GPUs](https://github.com/SJTU-IPADS/PowerInfer) [開源] PowerInfer 可在消費級 4090 上跨各種 LLM (包括 Llama 2 系列/Falcon-40B) 以平均超過 13 Tokens 速度進行推理，只比 A100 慢 18%

[Paper page - LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://huggingface.co/papers/2312.11514) [開源] 將超大模型放在 Flash 記憶體 (SSD?) 中推理的方法

[How to make LLMs go fast](https://vgel.me/posts/faster-inference/) 理解 LLM 加速之各種技術

[Time is Encoded in the Weights of Finetuned Language Models](https://browse.arxiv.org/html/2312.13401v1) [論文] 建立時間向量，微調 LLM 以針對週期性資訊有更佳推論成果

[Weight Subcloning: Direct Initialization of Transformers Using Larger Pretrained Ones](https://browse.arxiv.org/html/2312.09299v1) [論文] 權限壓縮複製為小模型以達成小模型快速遷移學習

[Discovering Latent Knowledge in Language Models Without Supervision](https://www.arxiv-vanity.com/papers/2212.03827/) [論文] 搜尋語言模型的內部知識，避免幻覺或說謊的問題

[Mapping the semantic void: Strange goings-on in GPT embedding spaces](https://www.lesswrong.com/posts/c6uTNm5erRrmyJvvD/mapping-the-semantic-void-strange-goings-on-in-gpt-embedding) GPT-J 的 4096 維空間 tokens 存在於兩個超球殼交集間。另先前發現的特異故障 tokens 多數接近質心 (也有遠離的)，可能因為訓練期的資料不多，因此接近於原始的隨機初始值

影像
----
[Imagen 2 on Vertex AI is now generally available](https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available) Google Imagen 2 企業級文字生影像模型，還可生成影像內文字及 logo，可以問答及支援多國語言

[Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models](https://sliders.baulab.info/) 漸變調整圖像生成樣式

[cumulo-autumn/StreamDiffusion: StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation](https://github.com/cumulo-autumn/StreamDiffusion) [開源] 即時影像生成

[TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering](https://jingyechen.github.io/textdiffuser2/) [開源] 包含高品質文字之圖像生成

多模態
------
[VideoPoet](https://sites.research.google/videopoet/) Google Research 推出可接受文字/圖像/影片輸入，生成短影片/音訊

[apple/ml-ferret](https://github.com/apple/ml-ferret) [開源] Apple 針對指定影像區域對話之非商業用途模型

[3D-GPT: 3D MODELING WITH LARGE LANGUAGE MODELS](https://chuny1.github.io/3DGPT/3dgpt.html) 以 LLM 指導 3D 建模框架

[Visualize Mixtral MoE](https://mixtral-moe-vis-d726c4a10ef5.herokuapp.com/) 視覺化理解多模態 Mixtral 領域專家分佈

[CyberRunner](https://www.cyberrunner.ai/) AI 學習控制實體彈珠迷宮破人類紀錄

工具
----
[microsoft/windows-ai-studio](https://github.com/microsoft/windows-ai-studio) Windows AI Studio 可下載模型、微調、測試，並在 Windows 程式中使用成果

[VRAM Calculator](https://vram.asmirnov.xyz/) 計算必要 VRAM 大小之線上工具

[h2oai/h2ogpt](https://github.com/h2oai/h2ogpt) [開源] 用本機 LLM 查詢本機文件工具

[Meet 'Coscientist,' your AI lab partner](https://new.nsf.gov/science-matters/meet-coscientist-your-ai-lab-partner) AI 搜尋化合物資訊，自行執行實驗並檢驗成品效果

趨勢
----
[Preparedness](https://openai.com/safety/preparedness) OpenAI 如何分級預備應對未來更強大的模型風險

[Advancements in machine learning for machine learning](https://blog.research.google/2023/12/advancements-in-machine-learning-for.html) Google 研究以機器學習方法提升機器學習效率

[AI Trends - Epoch](https://epochai.org/trends) 簡單地呈現許多重要的 AI 相關發展數據

[2023: The Year of AI. The most remarkable releases, partnerships, and legal debates](https://journal.everypixel.com/2023-the-year-of-ai) 介紹整年 AI 的重要成果

[NLP Research in the Era of LLMs - by Sebastian Ruder](https://nlpnewsletter.substack.com/p/nlp-research-in-the-era-of-llms) 提出五個不需大量運算的研究方向

[DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning](https://www.arxiv-vanity.com/papers/2006.08381/) [論文] 機器自行尋找出可解釋的演算法，還有睡眠模式由自行生成假想資料集離線學習

[The Scale of the Brain vs Machine Learning](https://www.beren.io/2022-08-06-The-scale-of-the-brain-vs-machine-learning/) 人腦與現今 AI 的運算規模類比，推估現今各 AI 與人腦各區域之規模差異及可能發展方向