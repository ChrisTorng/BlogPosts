# 企業級 LLM 叢集部署實戰 (DeepSeek R1 671B) 工作坊 課程錄影與心得

我參加了保哥的 [企業級 LLM 叢集部署實戰 (DeepSeek R1 671B)](https://learn.duotify.com/courses/ai-deploy) 課程，上午是觀念講解，下午是實戰。實戰中每個人都可以擁有兩台各八片 NVIDIA H100 主機，實際跑滿血版 DeepSeek R1 671B 模型。我參加了第二場實戰工作坊課程，第一次因硬體環境問題而失敗，於後來再補的課程中成功完成實作。我為了能充份運用個人獨佔的兩台主機，完成保哥的課前練習，還預備了壓測程式，想更多了解 LLM 的內部機制。在這個高貴又寶貴的課程中，我個人學到非常多，底下紀錄了我的學習心得。

個人雖然裝了 Ubuntu 一個多月，但多數也都只是用圖形介面。30 年前曾經學過很基礎的 Unix，當然都忘光了。

我完成了以下兩種環境的服務執行及 Open WebUI 連線:
- 3070 + WSL + Podman
- 4090 + Ubuntu + Podman + Proxy

底下列出我這次參考到的相關網頁:
- nVidia
  - [CUDA on WSL User Guide](https://docs.nvidia.com/cuda/wsl-user-guide/index.html)
  - [Support for Container Device Interface](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html)
- Podman
  - [GPU container access](https://podman-desktop.io/docs/podman/gpu)
  - [Accessing Podman from another WSL distribution](https://podman-desktop.io/docs/podman/accessing-podman-from-another-wsl-instance)



- 有關 `sudo apt-get update` 相關部份，我自己都用 `sudo apt update`。就我目前認知，`apt-get` 好像比較舊，`apt` 是新一代的指令。剛剛查到的參考文件 [Difference Between apt and apt-get Commands \[Explained\]](https://itsfoss.com/apt-vs-apt-get-difference/)。

- `mulitnode-vllm-sgl` 拼錯字，應該是 `multinode-vllm-sgl`，`it` vs `ti`。

- `multinode-vllm-sgl` 執行環境裡面若需要 `sudo`，密碼由 `Dockerfile` 裡面看到應該是 `SSH_PASSWORD` 的 `multinode1234567`，並非上課中用到的 `deepseek1234567`。

- 「測試 LLM 的 API」裡面提供的兩個指令，目前看都不完整，不確定先前是否是正確的? 我自己在 Ubuntu 裡使用:
  ```bash
  curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "google/gemma-3-1b-it",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"}
        ],
        "stream": true
    }'
  ```
  Windows 裡使用:
  ```cmd
  curl http://localhost:8000/v1/chat/completions  -H "Content-Type: application/json" -d "{\"model\": \"google/gemma-3-1b-it\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}], \"stream\": true}"
  ```

- Jupyter 步驟寫在啟動服務之後，我誤以為是要在已啟動 LLM 服務的狀態下再執行 Jupyter。還找到 LLM 服務執行中，按 [Ctrl+Z] 之後再執行 `bg` 指令的方式，還真的可以做到。不過建議把 Jupyter 寫在啟動服務之前，並標明是選擇性步驟。也可以加註提醒大家，上課中講師為大家事前準備的主機，實際上是停在這步。(可能還有額外的預先下載模型步驟?)

- Jupyter 執行中顯示出來的網址 http://127.0.0.1:8888/lab?token=61cdf263a40314364498b9006d5dd2c37aa9682170befe5d 直接就可以用來登入，應該不用特別再提取得 token，再開啟 http://127.0.0.1:8888/ 並填入 token 值這樣的步驟。

- Open WebUI 裡的 `OPENAI_API_BASE_URL` 參數，目前說明網頁內的 `"http://147.185.41.76/:8000/v1"` 一方面在 `/:8000` 前面多了一個 `/`，另可能有人沒注意直接貼上執行，錯誤 IP 預設值設進去，之後修改為正確參數也無法再變更了 (因為 volume 已建立，預設值無法再影響)。只能自行進入 Open WebUI - Admin Panel - Settigs - Connection 中修改。我想多數人自行練習時，會在同一台再開另一個 container 方式執行 Open WebUI，這裡我用 `localhost` 連自己也不通，最後試出來 `host.docker.internal` 才能通。

- 第一次執行 Open WebUI 時，可以再加上 `DEFAULT_MODELS` 參數，這樣就不需要再手動選擇模型，直接可交談了。

- 綜合以上兩點，我建議的指令為:
  ```bash
  podman run -d --name open-webui -p 3000:8080 \
    --add-host=host.docker.internal:host-gateway \
    -e OPENAI_API_BASE_URL="http://host.docker.internal:8000/v1" \
    -e OPENAI_API_KEY="dummy" \
    -e DEFAULT_MODELS="google/gemma-3-1b-it" \
    -v open-webui:/app/backend/data \
    --restart always \
    ghcr.io/open-webui/open-webui:main
  ```
  可以再加註可自行修改 `OPENAI_API_BASE_URL` 與 `DEFAULT_MODELS` 參數。

以上先跳過有關 WSL/Podman/Proxy 的部份。另 Grows.ai 部份我還沒做過，因此無法提供意見。

---

可惜我是做到最後還是沒有成功的，也沒機會進行壓測了。
先前上課及這次針對問題的解答，我還是沒有搞懂。
也許單純只是想太多，不過更需要的是基本觀念有待再補強，我再自己另外加強。
底下提供個人的學習項目。

在課前預備中，我學習到:

我有 WSL + Podman 及 Ubuntu + Podman 兩個環境。除了將指令中所有 `docker` 改為 `podman` 之外，還有:

首先是 WSL Podman 連線問題:
[Accessing Podman from another WSL distribution](https://podman-desktop.io/docs/podman/accessing-podman-from-another-wsl-instance)
我實際上是詢問 ChatGPT 後做出來的，實際步驟可能不比以上標準文件，這裡就不列出來了。

[CUDA on WSL User Guide](https://docs.nvidia.com/cuda/wsl-user-guide/index.html)
這裡說 WSL 不用安裝 nVidia 驅動，我實測執行 `nvidia-smi` 正常。

參考 [Support for Container Device Interface](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html)，要將這兩行:
```bash
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```
改為:
```bash
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
```
確認目前的 GPU
```bash
nvidia-ctk cdi list
=> INFO[0000] Found 2 CDI devices
   nvidia.com/gpu=0
   nvidia.com/gpu=all
```

但 WSL 目前仍卡在執行標準的指令失敗:
```bash
podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable ubuntu nvidia-smi -L
=> Error: preparing container a96f1e2456cd9333795c4c5f8b1b40f099eab1d159f793a31c313e49e789027b for attach: setting up CDI devices: unresolvable CDI devices nvidia.com/gpu=all
```

我的 Ubuntu 環境有成功。
---

在實戰中，我學習到:
`sudo su` 切換到超級使用者
