2024/1/11-1/20 AI 新知

語言
----
[Fast and Expressive LLM Inference with RadixAttention and SGLang](https://lmsys.org/blog/2024-01-17-sglang/) 重用 KV 快取以加速多次 LLM 推論的 RadixAttention

[Manifest AI - Linear Transformers Are Faster After All](https://manifestai.com/blogposts/faster-after-all/) (相較於非指數的) 線性 Transformer 還是更快 (但成果還沒有更好)

音訊
----
[WhisperSpeech](https://collabora.github.io/WhisperSpeech/) [開源] 反轉 Whisper 而得的文字轉語音模型 (還沒有中文)

知識
----
[Calculus on Computational Graphs: Backpropagation](https://colah.github.io/posts/2015-08-Backprop/) 介紹反向傳播的基本數學，讓我看得懂了

[Sampling for Text Generation](https://huyenchip.com/2024/01/16/sampling.html) 文字生成之參數詳細介紹

理論
----
[AlphaGeometry: An Olympiad-level AI system for geometry](https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/) Google DeepMind 的 AlphaGeometry 數學幾何 AI，與國際奧林匹克金牌選手能力相當，但目前只能解幾何問題

[WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation](https://arxiv.org/html/2312.14187v3) [論文] 以生成程式碼訓練程式碼型

[The Lazy Tyranny of the Wait Calculation](https://www.oneusefulthing.org/p/the-lazy-tyranny-of-the-wait-calculation) 現在立即動手解問題，不如等以後 AI 成熟了，能更快解決問題?